## Dockerとは？

> Docker（ドッカー）はコンテナ型の仮想化環境を提供するオープンソースソフトウェアである。

[Docker - Wikipedia](https://ja.wikipedia.org/wiki/Docker)

意味がわからないので噛み砕いて説明する(Wikipediaにマウントを取ろうとするオタクなので)

## 例として考えるシステム

ここでは例として、以下のようなシステムを動かす状況を考える。

1. ソフトウェアA: ライブラリBのバージョン1.0以下とライブラリCのバージョン3.0以上が必要
2. ソフトウェアD: ライブラリBのバージョン2.0以上が必要
3. ソフトウェアE: ソフトウェアAとソフトウェアCと通信する必要がある

(以下、このシステムを「システムS」と呼ぶことにする)

具体的に考えたければ、

- システムS → Webアプリケーションのバックエンド
- ソフトウェアA → データベース
- ライブラリB → ネットワーク処理ライブラリ
- ライブラリC → ファイル操作ライブラリ
- ソフトウェアD → サーバーソフト
- ソフトウェアE → 管理コントロールパネルアプリ

と置き換えるとイメージが湧きやすいと思う。

## 何が嬉しいのか

Dockerによってもたらされることの本質は、「環境を共有できること」で(あると私は考えてい)る。

何かソフトウェアを動かそうとした時に、環境構築に悩まされたことは誰しもある経験だと思う。
環境構築が必要なのは、本質的には「ソフトウェアが動作するために必要な要件を実行環境に課しているから」であると言える。例えばシステムSで言えば、ソフトウェアDは「ライブラリBのバージョン2.0以上が存在している」という要件をを環境に課している。そのため、ソフトウェアDを動作させるためにはまずライブラリBのバージョン2.0以上を環境に導入して、その要件を満たす必要がある。
しかし、(ライブラリ1つ程度ならまだしも)要件が複雑になっていくにつれて、動作させるための正確な環境を用意するのは難しくなってくる。システムSで言えば、ソフトウェアAがその一例である。多くの人がそのソフトウェアを使用する場合、その全員に同じ環境を用意してもらうのが骨の折れる作業であることは容易に想像がつくだろう。さらに特定の条件の環境でのみ発生するバグがあった場合、それを複数の開発者が再現させるのはさらに至難の技になる。

この問題(環境に対する動作要件が複雑だと困る、ということ)に対し、ソフトウェア開発者たちは様々なアプローチを行ってきた。例えば最近のパッケージマネージャは"lockfile"という、ソフトウェア間の依存関係を正確に記述しておくファイルを生成することが多くなっている([package-lock.json](https://docs.npmjs.com/files/package-lock.json), [yarn.lock](https://yarnpkg.com/lang/ja/docs/yarn-lock/), [Cargo.lock](https://doc.rust-lang.org/cargo/guide/cargo-toml-vs-cargo-lock.html)など)。この手法では、全てのユーザーが簡単なコマンドで(そのパッケージマネージャが管理する範囲内で)同じ環境を用意できるようになっている。

しかし、それでもこのアプローチ(lockfile)はそのパッケージマネージャが扱う範囲内でしか問題を解決してくれない。システムの深い部分 -ライブラリが依存しているCのライブラリなど- は、依然としてユーザーごとの環境に依存してしまっているため、ある程度は先述の問題が避けられなくなってしまう。

ここで登場するのがDockerだ。Dockerを使うと、「環境それ自体」を丸ごと配布(共有)できる。システムSで例えるならば、ソフトウェアAを配布する際に、ソフトウェアAの動作に必要なライブラリBとライブラリCを含んだ環境を配布するということだ。ユーザーはそれを使って、意図された環境で確実にソフトウェアAを動作させることができるのである。また利用者全員に同じ環境を配布しているので、バグの再現も容易だ。

別の言い方をすると、例えばソフトウェアAが「Ubuntu 18.04にパッケージXを入れた環境」で動作確認ができている場合、「Ubuntu 18.04にパッケージXを入れた環境」をそのままユーザーに配布すればみんな同じ環境で実行できるよね、というなかなか強引な考えなわけだ。とはいえ、Dockerのこのアプローチは世間で広く認められ、非常に広い分野で活用されている。

## VirtualBox, Vagrant, VMwareなどとの違い

技術に詳しい読者なら、VirtualBoxなどのイメージを配布すればいいではないかと思うかもしれない。しかしご存知の通り、これらハイパーバイザ型仮想化ソフトウェアで使用するイメージは巨大で、配布するのは一苦労だ(数GBのファイルを不特定多数に共有するのは大変でしょう)。さらにOSを丸ごと仮想化しているため、仮想環境の起動/終了に非常に時間がかかる。内部ではパソコンを起動/終了するのと同じことをシミュレートしているわけなので、これは避けられないコストだ。

Dockerがこれらの仮想化ソフトウェアと違う点は、OS(正確にはカーネル)をホスト環境と共有している点だ。これによってイメージを軽量に保つことができ([Ubuntu 18.10は29MB](https://hub.docker.com/_/ubuntu?tab=tags))、さらに起動/終了も高速に行うことができる。

## 不変性/再現性

Webアプリケーションなどの複雑なシステムでは、環境の一部を変更しただけでシステム全体の障害につながるということが簡単に起こりうる。Dockerの用意する「環境」は、それ自体不変のもので、変更を加えても容易に以前の状態に戻すことができる。これにより障害に対する対応が簡単になったり、開発の際にトライアンドエラーで試すといったような際の効率が圧倒的に上がったりする。(この辺りは試した方がよくわかると思うので、今は「ふーんそうなんだ」程度に思ってもらえれば構わない)

次章では、実際に手を動かしながらこれらの利点について体験してもらうと共に、Dockerを構成する基本概念について説明する。

[Dockerを使う](2-use-docker.md)
